{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Feed-Forward Network\n",
    "\n",
    "In this notebook we will play with Feed-Forward FC-NN (Fully Connected Neural Network) for a *classification task*: \n",
    "\n",
    "Image Classification on MNIST Dataset\n",
    "\n",
    "**RECALL**\n",
    "\n",
    "In the FC-NN, the output of each layer is computed using the activations from the previous one, as follows:\n",
    "\n",
    "$$h_{i} = \\sigma(W_i h_{i-1} + b_i)$$\n",
    "\n",
    "where ${h}_i$ is the activation vector from the $i$-th layer (or the input data for $i=0$), ${W}_i$ and ${b}_i$ are  the weight matrix and the bias vector for the $i$-th layer, respectively. \n",
    "<br><rb>\n",
    "$\\sigma(\\cdot)$ is the activation function. In our example, we will use the *ReLU* activation function for the hidden layers and *softmax* for the last layer.\n",
    "\n",
    "To regularize the model, we will also insert a Dropout layer between consecutive hidden layers. \n",
    "\n",
    "Dropout works by “dropping out” some unit activations in a given layer, that is setting them to zero with a given probability.\n",
    "\n",
    "Our loss function will be the **categorical crossentropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "Keras supports two different kind of models: the [Sequential](http://keras.io/models/#sequential) model and the [Graph](http://keras.io/models/#graph) model. The former is used to build linear stacks of layer (so each layer has one input and one output), and the latter supports any kind of connection graph.\n",
    "\n",
    "In our case we build a Sequential model with three [Dense](http://keras.io/layers/core/#dense) (aka fully connected) layers, with some [Dropout](http://keras.io/layers/core/#dropout). Notice that the output layer has the softmax activation function. \n",
    "\n",
    "The resulting model is actually a `function` of its own inputs implemented using the Keras backend. \n",
    "\n",
    "We apply the binary crossentropy loss and choose SGD as the optimizer. \n",
    "\n",
    "Please remind that Keras supports a variety of different [optimizers](http://keras.io/optimizers/) and [loss functions](http://keras.io/objectives/), which you may want to check out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ReLu** function is defined as $f(x) = \\max(0, x),$ [1]\n",
    "\n",
    "A smooth approximation to the rectifier is the *analytic function*: $f(x) = \\ln(1 + e^x)$\n",
    "\n",
    "which is called the **softplus** function.\n",
    "\n",
    "The derivative of softplus is $f'(x) = e^x / (e^x + 1) = 1 / (1 + e^{-x})$, i.e. the **logistic function**.\n",
    "\n",
    "[1] [http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf]() by G. E. Hinton "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Keep in mind this function as it is heavily used in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Deep Network according to the following requirements:\n",
    "\n",
    "```\n",
    "    FC@512+relu -> FC@512+relu -> FC@nb_classes+softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "nb_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Code Here\n",
    "\n",
    "# if you want to cheat:\n",
    "# %load ../solutions/sol_321.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (`keras.dataset`)\n",
    "\n",
    "We will train our model on the MNIST dataset, which consists of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. \n",
    "\n",
    "![](../imgs/mnist.png)\n",
    "\n",
    "Since this dataset is **provided** with Keras, we just ask the `keras.dataset` model for training and test data.\n",
    "\n",
    "We will:\n",
    "\n",
    "* download the data\n",
    "* reshape data to be in vectorial form (original data are images)\n",
    "* normalize between 0 and 1.\n",
    "\n",
    "The `binary_crossentropy` loss expects a **one-hot-vector** as input, therefore we apply the `to_categorical` function from `keras.utils` to convert integer labels to **one-hot-vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Put everything on grayscale\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, \n",
    "                                                  random_state=42, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels   :', np.asarray(range(10)))\n",
    "print('y_train_0:', Y_train[0].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_val[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels :', np.asarray(range(10)))\n",
    "print('y_val_0:', Y_val[0].astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Having defined and compiled the model, it can be trained using the `fit` function. We also specify a validation dataset to monitor validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=2, verbose=1, \n",
    "                            validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Network Performance Trend\n",
    "The return value of the `fit` function is a `keras.callbacks.History` object which contains the entire history of training/validation loss and accuracy, for each epoch. We can therefore plot the behaviour of loss and accuracy during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['acc'])\n",
    "    plt.plot(network_history.history['val_acc'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(network_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `2` epochs, we get a `~88%` validation accuracy. \n",
    "\n",
    "* If you increase the number of epochs, you will get definitely better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Exercise: \n",
    "\n",
    "Try increasing the number of epochs (if you're hardware allows to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=SGD(lr=0.001), \n",
    "              metrics=['accuracy'])\n",
    "network_history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=50, verbose=1, \n",
    "                            validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dropout layers** have the very specific function to *drop out* a random set of activations in that layers by setting them to zero in the forward pass. \n",
    "\n",
    "Simple as that. \n",
    "\n",
    "It allows to avoid *overfitting* but has to be used **only** at training time and **not** at test time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "\n",
    "Applies Dropout to the input.\n",
    "\n",
    "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "Arguments\n",
    "\n",
    "* rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "* noise_shape: 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape  (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
    "* seed: A Python integer to use as random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Keras guarantees automatically that this layer is **not** used in **Inference** (i.e. Prediction) phase\n",
    "(thus only used in **training** as it should be!)\n",
    "\n",
    "See `keras.backend.in_train_phase` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "\n",
    "## Pls note **where** the `K.in_train_phase` is actually called!!\n",
    "Dropout??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.in_train_phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try modifying the previous example network adding a Dropout layer:\n",
    "\n",
    "```\n",
    "    FC@512+relu -> DropOut(0.2) -> FC@512+relu -> DropOut(0.2) -> FC@nb_classes+softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Code Here.. or cheat, if you want to:\n",
    "# %load ../solutions/sol_312.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network_history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=50, verbose=1, validation_data=(X_val, Y_val))\n",
    "plot_history(network_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you continue training, at some point the validation loss will start to increase: that is when the model starts to **overfit**. \n",
    "\n",
    "It is always necessary to monitor training and validation loss during the training of any kind of Neural Network, either to detect overfitting or to evaluate the behaviour of the model **(any clue on how to do it??)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already used `summary`\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.layers` is iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Input Tensors: ', model.input, end='\\n\\n')\n",
    "print('Layers - Network Configuration:', end='\\n\\n')\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    print('Layer Configuration:')\n",
    "    print(layer.get_config(), end='\\n{}\\n'.format('----'*10))\n",
    "print('Model Output Tensors: ', model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden layer representation of the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **simple** way to do it is to use the weights of your model to build a new model that's truncated at the layer you want to read. \n",
    "\n",
    "Then you can run the `._predict(X_batch)` method to get the activations for a batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_truncated = Sequential()\n",
    "model_truncated.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model_truncated.add(Dropout(0.2))\n",
    "model_truncated.add(Dense(512, activation='relu'))\n",
    "\n",
    "for i, layer in enumerate(model_truncated.layers):\n",
    "    layer.set_weights(model.layers[i].get_weights())\n",
    "\n",
    "model_truncated.compile(loss='categorical_crossentropy', optimizer=SGD(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "np.all(model_truncated.layers[0].get_weights()[0] == model.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_features = model_truncated.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: Alternative Method to get activations \n",
    "\n",
    "(Using `keras.backend` `function` on Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def get_activations(model, layer, X_batch):\n",
    "    activations_f = K.function([model.layers[0].input, \n",
    "                                K.learning_phase()], \n",
    "                               [layer.output,])\n",
    "    activations = activations_f((X_batch, False))\n",
    "    return activations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Embedding of Hidden Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(hidden_features[:1000]) ## Reduced for computational issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_map = np.argmax(Y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(colors_map==6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([x for x in 'b-g-r-c-m-y-k-purple-coral-lime'.split('-')])\n",
    "colors_map = colors_map[:1000]\n",
    "plt.figure(figsize=(10,10))\n",
    "for cl in range(nb_classes):\n",
    "    indices = np.where(colors_map==cl)\n",
    "    plt.scatter(X_tsne[indices,0], X_tsne[indices, 1], \n",
    "                c=colors[cl], label=cl)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bokeh (Interactive Chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To install `bokeh`):\n",
    "\n",
    "```shell\n",
    "$ conda install bokeh\n",
    "```\n",
    "\n",
    "**OR**:\n",
    "\n",
    "```shell\n",
    "$ pip install bokeh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.resources import CDN\n",
    "\n",
    "output_notebook(CDN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=600, plot_height=600)\n",
    "\n",
    "colors = [x for x in 'blue-green-red-cyan-magenta-yellow-black-purple-coral-lime'.split('-')]\n",
    "colors_map = colors_map[:1000]\n",
    "for cl in range(nb_classes):\n",
    "    indices = np.where(colors_map==cl)\n",
    "    p.circle(X_tsne[indices, 0].ravel(), X_tsne[indices, 1].ravel(), size=7, \n",
    "             color=colors[cl], alpha=0.4, legend=str(cl))\n",
    "\n",
    "# show the results\n",
    "p.legend.location = 'bottom_right'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We used `default` TSNE parameters. Better results can be achieved by tuning TSNE Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: \n",
    "\n",
    "### Try with a different algorithm to create the manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "\n",
    "### Try extracting the Hidden features of the First and the Last layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try using the `get_activations` function relying on keras backend\n",
    "def get_activations(model, layer, X_batch):\n",
    "    activations_f = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\n",
    "    activations = activations_f((X_batch, False))\n",
    "    return activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
