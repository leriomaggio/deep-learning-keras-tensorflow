{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “In God we trust. All others must bring data.” – W. Edwards Deming, statistician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "### What?\n",
    "Convert words to vectors in a high dimensional space. Each dimension denotes an aspect like gender, type of object / word.\n",
    "\n",
    "### Why?\n",
    "By converting words to vectors we build relations between words. More similar the words in a dimension, more closer their scores are.\n",
    "\n",
    "### Example\n",
    "_W(green) = (1.2, 0.98, 0.05, ...)_\n",
    "\n",
    "_W(red) = (1.1, 0.2, 0.5, ...)_\n",
    "\n",
    "Here the vector values of _green_ and _red_ are very similar in one dimension because they both are colours. The value for second dimension is very different because red might be depicting something negative in the training data while green is used for positiveness.\n",
    "\n",
    "By vectorizing we are indirectly building different kind of relations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading blog post from data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/valerio/Projects/repos/deep-learning-keras-euroscipy2016/data\n"
     ]
    }
   ],
   "source": [
    "DATA_DIRECTORY = os.path.join(os.path.abspath(os.path.curdir), 'data')\n",
    "print(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_posts = []\n",
    "female_post = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIRECTORY,\"male_blog_list.txt\"),\"rb\") as male_file:\n",
    "    male_posts= pickle.load(male_file)\n",
    "    \n",
    "with open(os.path.join(DATA_DIRECTORY,\"female_blog_list.txt\"),\"rb\") as female_file:\n",
    "    female_posts = pickle.load(female_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2252\n",
      "2611\n"
     ]
    }
   ],
   "source": [
    "print(len(female_posts))\n",
    "print(len(male_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "```from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_male_posts = list(filter(lambda p: len(p) > 0, male_posts))\n",
    "filtered_female_posts = list(filter(lambda p: len(p) > 0, female_posts))\n",
    "posts = filtered_female_posts + filtered_male_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247 2595 4842\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_female_posts), len(filtered_male_posts), len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(size=200, min_count=1)\n",
    "w2v.build_vocab(map(lambda x: x.split(), posts[:100]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.similarity('I', 'My')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(posts[5])\n",
    "w2v.similarity('ring', 'husband')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "\n",
    "The same technique of word2vec is extrapolated to documents. Here, we do everything done in word2vec + we vectorize the documents too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 for male, 1 for female\n",
    "concatenate_array = np.concatenate((np.zeros(len(filtered_male_posts)),\n",
    "                                    np.ones(len(filtered_female_posts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(concatenate_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create cross validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.concatenate((filtered_male_posts,filtered_female_posts)),\n",
    "                                                    concatenate_array, \n",
    "                                                    test_size=0.2) # 80% training, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape[0], y_train.shape[0], X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelizeReviews(reviews, label_type):\n",
    "    \"\"\"\"\"\"\n",
    "    labelized = []\n",
    "    for i,v in enumerate(reviews):\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v,[label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_label = labelizeReviews(X_train,'TRAIN')\n",
    "X_test_label = labelizeReviews(X_test,'TEST')\n",
    "\n",
    "print(len(x_train_label),len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have labelized reviews, now building Doc2Vec models using Distibuted Memory (DM) and Distributed Bag of Words (DBoW)\n",
    "\n",
    "* **DM** - Given the context (set of paragraphs), predict the next word\n",
    "* **DBoW** - Given the word, predict the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dm defines the training algorithm. By default (dm=1), distributed memory is used. Otherwise, dbow is employed.\n",
    "\n",
    "#size is the dimensionality of the feature vectors.\n",
    "\n",
    "#window is the maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "#alpha is the initial learning rate (will linearly drop to zero as training progresses).\n",
    "\n",
    "#seed = for the random number generator.\n",
    "\n",
    "#min_count = ignore all words with total frequency lower than this.\n",
    "\n",
    "#sample = threshold for configuring which higher-frequency words are randomly downsampled;\n",
    "#default is 0 (off), useful value is 1e-5.\n",
    "#workers = use this many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "#hs = if 1 (default), hierarchical sampling will be used for model training (else set to 0).\n",
    "\n",
    "#negative = if > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20).\n",
    "\n",
    "#dm_mean = if 0 (default), use the sum of the context word vectors. If 1, use the mean. Only applies when dm is used.\n",
    "\n",
    "model_dm = gensim.models.Doc2Vec(min_count=1,window=10,size=size,sample=1e-3,negative=5,workers=20)\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=1,window=10,size=size,sample=1e-3,negative=5,workers=20,dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dm.build_vocab(np.concatenate((X_train_label, X_test_label)))\n",
    "model_dbow.build_vocab(np.concatenate((X_train_label, X_test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_label_np = np.array(X_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_label_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(X_train_label_np.shape[0])\n",
    "    model_dm.train(X_train_label_np[perm])\n",
    "    model_dbow.train(X_train_label_np[perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVecs(model,corpus,size):\n",
    "    \"\"\"\"\"\"\n",
    "    vecs = [np.array(model[z.labels[0]]).reshape((1,size)) for z in corpus]\n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs_dm = getVecs(model_dm, X_train_label_np, size)\n",
    "train_vecs_dbow = getVecs(model_dbow, X_train_label_np, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_label_np = np.array(X_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(X_test_label_np.shape[0])\n",
    "    model_dm.train(X_test_label_np[perm])\n",
    "    model_dbow.train(X_test_label_np[perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vecs_dm = getVecs(model_dm, X_test_label_np, size)\n",
    "test_vecs_dbow = getVecs(model_dbow, X_test_label_np, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))\n",
    "print(test_vecs_dm.shape, test_vecs_dbow.shape, male_female_train.shape, male_female_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have all the vectors now, we have to train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD classifier with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrl1 = SGDClassifier(loss='log', penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrl1.fit(train_vecs, male_female_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy : %.2f' % lrl1.score(test_vecs,male_female_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD classifier with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrl2 = SGDClassifier(loss='log', penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrl2.fit(train_vecs,male_female_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy : %.2f' % lrl2.score(test_vecs,male_female_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_l1_kf = KFold(n=train_vecs.shape[0], n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_l1_kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_vecs_df = pd.DataFrame(train_vecs)\n",
    "target_np = np.array(male_female_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_vecs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_vecs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_l1_kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_l1_metrics = []\n",
    "for train_index, validate_index in sgd_l1_kf:\n",
    "    sample_train,sample_validate = trained_vecs_df.loc[train_index],trained_vecs_df.loc[validate_index]\n",
    "    \n",
    "    sample_train_target,sample_validate_target = male_female_train[train_index],male_female_train[validate_index]\n",
    "    \n",
    "    #print(sample_train.shape, sample_validate.shape, sample_train_target.shape, sample_validate_target.shape)\n",
    "    \n",
    "    sgd_l1 = SGDClassifier(loss='log', penalty='l1')\n",
    "    \n",
    "    sgd_l1.fit(sample_train,sample_train_target)\n",
    "    \n",
    "    sgd_l1_predicted = sgd_l1.predict(sample_validate)\n",
    "    \n",
    "    sgd_l1_predicted_copy = sgd_l1_predicted.copy()\n",
    "    \n",
    "    sgd_l1_predicted[sgd_l1_predicted > 0.5] = 1\n",
    "    sgd_l1_predicted[sgd_l1_predicted <= 0.5] = 0\n",
    "  \n",
    "    \n",
    "    sgd_l1_analysis = pd.concat([pd.Series(sample_validate_target),pd.Series(sgd_l1_predicted)],axis=1)\n",
    "\n",
    "    sgd_l1_analysis.columns = ['actual','prediction']\n",
    "    \n",
    "    sgd_l1_auc = metrics.roc_auc_score(sgd_l1_analysis.actual,sgd_l1_analysis.prediction)\n",
    "        \n",
    "    sgd_l1_metrics.append((sgd_l1_auc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_l1_metrics_df = pd.DataFrame(sgd_l1_metrics).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(sgd_l1_analysis.actual, sgd_l1_predicted_copy)\n",
    "pyplot.plot(fpr, tpr)\n",
    "pyplot.plot([0,1],[0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
